<div class="container">
  <h2>ü§ñ What is Q-Learning?</h2>
  <p>
    Q-Learning is a model-free reinforcement learning algorithm used to learn the optimal action-selection policy for an agent in a Markov Decision Process (MDP).  
    It helps an agent learn how to act optimally in an environment by interacting with it, receiving rewards, and updating its knowledge of action values over time.
  </p>

  <h2>üìå Core Idea</h2>
  <p>
    Q-Learning uses a table (called the <strong>Q-table</strong>) to estimate the utility (or "Q-value") of taking a particular action in a given state.  
    The goal is to learn the optimal Q-values which guide the agent to take the best actions for maximizing cumulative rewards.
  </p>

  <h2>üßÆ Q-Learning Formula</h2>
  <div class="formula-box">
    <p>
      <strong>Q(s, a) ‚Üê Q(s, a) + Œ± [R + Œ≥ max<sub>a'</sub> Q(s', a') ‚àí Q(s, a)]</strong>
    </p>
  </div>
  <p>
    <strong>Where:</strong><br>
    <ul>
      <li><strong>Q(s, a):</strong> Current Q-value of state <code>s</code> and action <code>a</code></li>
      <li><strong>Œ± (alpha):</strong> Learning rate (how quickly we adopt new knowledge)</li>
      <li><strong>Œ≥ (gamma):</strong> Discount factor (how much we value future rewards)</li>
      <li><strong>R:</strong> Immediate reward from taking action <code>a</code> in state <code>s</code></li>
      <li><strong>s‚Ä≤:</strong> Next state after taking action <code>a</code></li>
      <li><strong>max<sub>a‚Ä≤</sub> Q(s‚Ä≤, a‚Ä≤):</strong> Best possible Q-value in the next state</li>
    </ul>
  </p>

  <h2>üìà Visualization</h2>
  <div class="image-box">
    <img src="../images/qlearning.png" alt="Q-learning visualization (agent learning)">
    <p><em>Agent interacts with the environment, collects rewards, and updates Q-values to learn optimal policy.</em></p>
  </div>

  <h2>üöÄ Applications of Q-Learning</h2>
  <ul>
    <li>Game AI (e.g., Tic-Tac-Toe, Chess, GridWorld)</li>
    <li>Robot navigation and control systems</li>
    <li>Recommendation systems</li>
    <li>Traffic signal control and autonomous vehicles</li>
    <li>Portfolio and stock trading strategies</li>
  </ul>

  <h2>‚úÖ Advantages</h2>
  <ul>
    <li>Does not require a model of the environment (model-free)</li>
    <li>Simple to implement and understand</li>
    <li>Works well in discrete environments with clear state-action definitions</li>
  </ul>

  <h2>‚ö†Ô∏è Limitations</h2>
  <ul>
    <li>Scalability issues for large state-action spaces (needs function approximation like Deep Q-Learning)</li>
    <li>Can converge slowly without proper exploration (e.g., Œµ-greedy strategy)</li>
    <li>Performance depends on tuning learning rate (Œ±) and discount factor (Œ≥)</li>
  </ul>
</div>

<style>
  .container {
    max-width: 900px;
    margin: 40px auto;
    background: #ffffff;
    padding: 30px;
    border-radius: 12px;
    box-shadow: 0 4px 15px rgba(0,0,0,0.1);
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    color: #2c3e50;
  }

  h2 {
    margin-top: 30px;
    color: #34495e;
  }

  p, li {
    font-size: 1.15em;
    line-height: 1.6;
  }

  ul {
    margin-left: 20px;
  }

  .formula-box {
    text-align: center;
    margin: 20px 0;
    background-color: #f7f9fa;
    padding: 15px;
    border-left: 4px solid #3498db;
    font-size: 1.3em;
    font-weight: bold;
    color: #2c3e50;
  }

  .image-box {
    margin: 25px 0;
    text-align: center;
  }

  .image-box img {
    max-width: 100%;
    height: auto;
    border-radius: 10px;
    box-shadow: 0 2px 10px rgba(0,0,0,0.15);
  }

  .image-box p {
    margin-top: 8px;
    font-style: italic;
    color: #7f8c8d;
    font-size: 1em;
  }
</style>
