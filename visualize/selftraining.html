<div class="container">
  <h2>ü§ñ What is Self-Training Semi-Supervised Learning?</h2>
  <p>
    Self-training is a simple yet powerful technique used in <strong>semi-supervised learning</strong>, where the model learns from a small labeled dataset and leverages a larger unlabeled dataset to improve its performance.  
    The main idea is to iteratively expand the labeled data by using the model‚Äôs own confident predictions as "pseudo-labels" on the unlabeled data.
  </p>

  <h2>üìö Detailed Concept</h2>
  <p>
    Initially, a supervised model is trained using the labeled data available. This model is then used to predict labels on the unlabeled dataset.  
    From these predictions, only those with high confidence are selected and treated as labeled data (pseudo-labels). These pseudo-labeled samples are added to the labeled dataset, and the model is retrained on this larger dataset.  
    This cycle repeats, progressively improving the model‚Äôs accuracy by making use of previously unlabeled data.
  </p>
  
  <h2>üß© Uses of Self-Training</h2>
  <ul>
    <li>To reduce the amount of manual labeling required in supervised learning.</li>
    <li>Improve model performance when labeled data is scarce but unlabeled data is abundant.</li>
    <li>Useful in scenarios where labeling is expensive, time-consuming, or requires expert knowledge.</li>
  </ul>

  <h2>üöÄ Applications</h2>
  <ul>
    <li><strong>Natural Language Processing:</strong> Text classification, sentiment analysis, named entity recognition.</li>
    <li><strong>Computer Vision:</strong> Image classification, object detection with limited labeled images.</li>
    <li><strong>Speech Recognition:</strong> Improving accuracy using unlabeled audio data.</li>
    <li><strong>Healthcare:</strong> Medical image analysis where labeled data is limited due to privacy and expertise.</li>
  </ul>

  <h2>üñºÔ∏è Visualization</h2>
  <div class="image-box">
    <img src="../images/selftraining.png" alt="Self-training semi-supervised learning workflow">
    <p><em>Visualization of Self-Training Workflow: The model trains on labeled data, predicts on unlabeled data, selects confident predictions as pseudo-labels, then retrains on the expanded dataset.</em></p>
  </div>

  <h2>‚úÖ Advantages</h2>
  <ul>
    <li>Efficiently uses unlabeled data, reducing labeling costs.</li>
    <li>Simple and easy to implement with existing supervised models.</li>
    <li>Can significantly improve performance over purely supervised learning in low-label scenarios.</li>
  </ul>

  <h2>‚ö†Ô∏è Limitations</h2>
  <ul>
    <li>Incorrect pseudo-labels can accumulate, causing error propagation and model degradation.</li>
    <li>Requires careful tuning of confidence thresholds to select reliable pseudo-labels.</li>
    <li>Can be sensitive to the initial model quality ‚Äî poor initial models reduce effectiveness.</li>
    <li>May need monitoring and early stopping to avoid overfitting on noisy pseudo-labeled data.</li>
  </ul>
</div>

<style>
  .container {
    max-width: 900px;
    margin: 40px auto;
    background: #fff;
    padding: 30px;
    border-radius: 12px;
    box-shadow: 0 4px 15px rgba(0,0,0,0.1);
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    color: #2c3e50;
  }

  h2 {
    margin-top: 30px;
    color: #34495e;
  }

  p, li {
    font-size: 1.15em;
    line-height: 1.6;
  }

  ul {
    margin-left: 20px;
  }

  .image-box {
    margin: 25px 0;
    text-align: center;
  }

  .image-box img {
    max-width: 100%;
    height: auto;
    border-radius: 10px;
    box-shadow: 0 2px 10px rgba(0,0,0,0.15);
  }

  .image-box p {
    margin-top: 8px;
    font-style: italic;
    color: #7f8c8d;
    font-size: 1em;
  }
</style>
